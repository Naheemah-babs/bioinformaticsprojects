
```{r}
# Load dataset
ds1 <- read.csv("data1.csv", row.names = 1)
ds1
```
```{r}
# Check its dimensions. Check out the documentation with "?dim"
dim(ds1)
```
```{r}
# to check the data
head(ds1)
```
```{r}
# Access third, fifth, and 99th row directly.
ds1[c(3,5,99), ]
```
```{r}
#identifying patients with more than 100 in weight 
idx <- ds1$Weight > 100
```
```{r}
#exploring the results
table(idx)
```
```{r}
#selecting samples of interest
rownames(ds1)
ds1$Weight > 100
```
```{r}
### Lets use the vector to filter the rownames of interest.
rownames(ds1)[ ds1$Weight > 100 ]
```
```{r}
rownames(ds1)
```
```{r}
## checking how many patients are experiencing in average exactly 10 hours of sun per day?
table(ds1$Hours_Sun == 10)
```
```{r}
## comparing two columns (Hours in the sun and sleep hours)
table(ds1$Hours_Sun > ds1$Sleep_Hours)
```
```{r}
## checking how many patients have been in the hospital from 2 to 4 times?
table(ds1$Hospital_Visits %in% 2:4)
## or
table(ds1$Hospital_Visits >= 2 & ds1$Hospital_Visits <= 4)
```
```{r}
table(ds1$Hospital_Visits >= 4 & ds1$Weight > 100)
```
```{r}
## finding patients which are more than 100 kg in weight with more than 3 hospitalizations or are older than 80 years of age.
table((ds1$Hospital_Visits >= 4 & ds1$Weight > 100) | ds1$Age >= 80)
```
```{r}
hist(ds1$Hours_Sun,breaks=10)
```
```{r}
#filtering out individuals with unrealistic sun hours
ind_wrong_values <- ds1$Hours_Sun < 3
ind_wrong_values
```
```{r}
#storing realistic values to a new file
ds1_filt <- ds1[!ind_wrong_values,]
ds1_filt
```



##### Quality Control of data



```{r}
## create an index pointing out individuals of age 105 years
idx <- which(ds1$Age == 105)
ds1[idx, ]
```

```{r}
## iterate over all columns of our dataset and test if there are any NaN contained
for(i in 1:ncol(ds1)) 
{
  if(any(is.nan(ds1[,i])))
  {
    cat('There are ' , length(is.nan(ds1[,i])), 'NaNs in column ', colnames(ds1)[i], '\n')
  } else
  {
    cat('There are no NaNs in column ', colnames(ds1)[i], '\n')
  }
}
```

#### Exploratory Data Analysis

```{r}
library(tidyverse)
library(ggplot2)
```

```{r}
ds2 <- read.csv("data2.csv", row.names = 1)
ds2 <- ds2[complete.cases(ds2),] ##complete cases checks missing values
```

```{r}
summary(ds2)
```

```{r}
ggplot(ds2, aes(x=Weight, y=LDL))  + 
    geom_point() + 
    geom_smooth(formula = y ~ x, method="lm", se=FALSE) +
    # axis limits
    xlim(c(0, 350)) + 
    ylim(c(0, 300))+
    # and change title and axis labels
    ggtitle("Weight vs. LDL") +
    xlab("Body Weight") + ylab("LDL level")
```

## A good way to visualize a third aspect of the data in a scatter plot 
 is to represent the third value as color of the points.
```{r}
options(repr.plot.width = 4.5, repr.plot.height = 3)

ggplot(ds2, aes(x=Weight, y=LDL))  + 
      geom_point(aes(col=Hospital_Visits), size=3) +
    ggtitle("Weight vs. LDL") +
    xlab("Body Weight") + 
    ylab("LDL level")
```


# Visualization additional plots 

## Histogram: visualize the empirical distribution of a random variable.
```{r}
ggplot(ds2, aes(x = Age)) + 
	geom_histogram(binwidth = 5) +
	xlab("Weight") +
	ylab("Patients")
```
## Boxplot: comparing distributions of variables 
```{r}
ggplot(ds2, aes(y=Weight, fill=Color_House))  + 
	geom_boxplot() +
	ylab("Weight") +
	ggtitle("Count Distribution per Sample") +
	theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
```
# Principal Component Analysis 
```{r}
idx <- grep("Color_House", colnames(ds2)) 
ds2.onlyNumerical <- ds2[,-idx]

pca <- prcomp(ds2.onlyNumerical, scale = TRUE, center = TRUE)

summary(pca)
```

The summary gives us a good overview over the PCA result.

- row "Proportion of Variance" corresponds to the amount of variance the component accounts for in the data. Accordingly, PC1 accounts for about 31% of the total variance in the data, while PC2 accounts for 14%!
- row "Cumulative Proportion" is the cumulative proportion of variance explained by the first x components. The first 5 components explain about 83% of the total variation in the dataset

##Lets visualize this summary.
```{r}
# Visualize Eigenvalues 
options(repr.plot.width = 5, repr.plot.height = 5)
plot(pca,
	xlab = "Dimension",
	main = 'Scree plot')

# Cumulative explained variability plot
cp <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cp, 
    xlab = "PC #", 
     ylab = "Amount of explained variance", 
     main = "Cumulative variance plot")

# Visualize individual patients using only the first two components
plot(pca$x[,1], pca$x[,2], 
	xlab = 'PC 1', 
	ylab = 'PC 2', 
	main = 'PCA across Patients')
```
## Do all of the rows have the same variance independent of the mean counts?

```{r}
# Let's make a plot detailing the contributions of each variable to the first two components of our PCA
install.packages("factoextra")
library("factoextra")
options(repr.plot.width = 10, repr.plot.height = 10)
fviz_pca_var(pca, geom = c("point", "text"))
```
# Clustering

```{r}
# contructing a distance matrix between all sample combinations
ds2.onlyNumerical.sc <- scale(ds2.onlyNumerical, scale = TRUE, center = TRUE)
dist.by.variable  <- dist(t(ds2.onlyNumerical.sc))

# Hierarchical clustering
dt.clust.by.var <- hclust(dist.by.variable)

# Plot a dendrogram
options(repr.plot.width = 5, repr.plot.height = 5)
plot(dt.clust.by.var)
```

## Heatmaps & Clustering

Lets try to cluster our patients and visualize them as heatmap.

```{r}
# Calculated the distances and then the hierarchical clustering
dist.by.patient <- dist(ds2.onlyNumerical.sc)
dt.clust.by.patient <- hclust(dist.by.patient)


options(repr.plot.width = 4, repr.plot.height = 7)

# load package to make heatmaps
install.packages("pheatmap")
library(pheatmap)

# and finally plot the count values as heatmap
options(repr.plot.width = 6, repr.plot.height = 15)
pheatmap(ds2.onlyNumerical.sc,
    show_rownames = F,
    cluster_rows = dt.clust.by.patient,
    cluster_cols = dt.clust.by.var)
```
# Correlation.

# How to quantify similarity between variables?
## Visual exploration
A scatterplot suggests a relationship between our variables Hours_Sun and LDL. 
```{r}
ggplot(ds1, aes(x=Hours_Sun, y=LDL)) + 
    geom_point()
```
Â´ManualÂ´ calculation of covariance
```{r}
Hours_sun_LDL_cov <- sum((ds1$Hours_Sun - mean(ds1$Hours_Sun)) * (ds1$LDL - mean(ds1$LDL))) / (nrow(ds1) -1)
round(Hours_sun_LDL_cov, 2)
```
## How do we quantify such a relationship? Covariance
 built-in function of R for the covariance: cov
```{r}
Hours_sun_LDL_cov <- cov(ds1$Hours_Sun, ds1$LDL)
round(Hours_sun_LDL_cov, 2)

```
# Computing the correlation 

## Manual calculation of correlation, a pearson correlation of 0 means there is no relationship, 1 means there is a perfect relatonship while -1 meabs a negative relationship
```{r}
Hours_sun_LDL_cov <- sum((ds1$Hours_Sun - mean(ds1$Hours_Sun)) * (ds1$LDL - mean(ds1$LDL))) / (nrow(ds1) -1)
Hours_sun_LDL_cor <- Hours_sun_LDL_cov / (sd(ds1$Hours_Sun) * sd(ds1$LDL))
cat("Pearson correlation Hours_sun vs. LDL: ", round(Hours_sun_LDL_cor, 3), '\n')
```
## R built-in function for the correlation
```{r}
Hours_sun_LDL_cor <- cor(ds1$Hours_Sun, ds1$LDL)
Hours_sun_LDL_cor
```
## Statistical analysis.

The alternative hypothesis is defined as the correlation being different to 0 

```{r}
cor.test(ds1$Hours_Sun, ds1$LDL)
```

## Statistical analysis: alternative tests.
We can specify the alternative hypothesis, e.g. that the correlation is larger than 0.

```{r}
cor.test(ds1$Hours_Sun, ds1$LDL, alternative = 'greater')
```
# Assumptions for correlation.
## Gaussian/Normal distributions for each variable.
```{r}
shapiro.test(ds1$Hours_Sun)
shapiro.test(ds1$LDL)
```
for the shapiro for HOURS-SUN test, we fail to reject the null hypothesis that the data follows a normal distribution because the p-value is greater than 0.05, meaning that the LDL is Gaussian(does follow normal distribution)
for the shapiro for LDL test, we reject the null hypothesis that the data follows a normal distribution because the p-value is less than 0.05, meaning that the LDL is not Gaussian(does not follow normal distribution)
## Non-parametric correlation analysis.

```{r}
cor.test(ds1$Hours_Sun, ds1$LDL,method="spearman")
```
because the p-value gotten in sapiro test for LDL is low, we may reject the hypothesis that the LDL, it's Gaussian distributed. If that's the case, then the assumptions for computing Pearson correlation are not fulfilled, and alternatives should be considered. Alternatives are non-parametric correlation, when we are using the ranking inside of the original value,using Spearman correlation. Spearman correlation, we also get a p-value that is extremely small, quite small, and therefore we are able to reject the null hypothesis, 

## Fit a linear model
```{r}
wt.vs.ldl.lm <- lm(Weight ~ LDL, ds1)
```

## Print the summary of the model
```{r}
summary(wt.vs.ldl.lm)
```

## What to look into...

- The estimated intercept and slope are -156.8 and 1.9. 
- The result looks good, the adjusted R-square value of 0.9992 is close to 1.
- The definition of R-squared is the percentage of the response
      variable variation that is explained by a linear model. 
      
      R-squared = Explained variation / Total variation 
      R-squared is always between 0 and 1.

## Access the values
```{r}
s <- summary(wt.vs.ldl.lm)
s$r.squared
```


# Evaluate the linear model

## Let's inspect the result visually. 

First with a simple plot of our two variables, and the linear fit.
```{r}
plt <- ds1 %>% 
  ggplot(aes(x = LDL, y = Weight)) + 
  geom_point() +
  xlab("LDL") +
  ylab("Weight") +
geom_smooth(method='lm', se = FALSE)  
plt
```
As not all data points are perfectly captured by the linear model, 
we should inspect the residuals with respect to the fitted values. 

## Distribution of the residuals
```{r}
mean(wt.vs.ldl.lm$residuals)

hist(wt.vs.ldl.lm$residuals)
```
## QQ-plot
Comparing the residual distribution to a theoretical normal distribution 
in a quantile-quantile-plot (QQ-plot).

```{r}
plot(wt.vs.ldl.lm, 2)
```


## Fitted value vs Residuals
Another intuitive way is to examine the relationship between the fitted value and 
the corresponding residual. We see that the model systematically deviates from 
the data, overestimating the weight for very low and high LDL values, and slightly 
underestimates it for intermediate values.
```{r}
options(repr.plot.width = 6, repr.plot.height = 6)
plot(wt.vs.ldl.lm, 1)
```